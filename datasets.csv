Dataset Name,Website Link,Paper Link,Year,Task,Event Types,Event/Video Count,Instantiation,Genre/Video purpose,Text/Graphics,Modalities,Annotations (+ Event structure),Motivation/Contribution (excerpt from paper)
TRECVID,https://trecvid.nist.gov/trecvid.data.html#tv23,https://dl.acm.org/doi/abs/10.1145/1178677.1178722,2006,Retrieval,Current events,"17K videos, 24 movies",Unknown,News footage,T,Video,"temporal intervals, retrieval queries, various high level features","The TREC Video Retrieval Evaluation (TRECVid)is an international benchmarking activity to encourage research in video information retrieval by providing a large test collection, uniform scoring procedures, and a forum for organizations 1 interested in comparing their results."
CCV,https://www.ee.columbia.edu/ln/dvmm/CCV/,https://www.ee.columbia.edu/ln/dvmm/publicationPage//Publi//icmr11:consumervideo.html,2011,Classification,Daily life,"9,317 YouTube videos over 20 semantic categories",Unknown,Internet videos,F,Video,Class labels,"Existing corpora for video analysis lack scale and/or content diversity, and thus limited the needed progress in this critical area. In this paper, we describe and release a new database called CCV, containing 9,317 web videos over 20 semantic categories, including events like ""baseball"" and ""parade"", scenes like ""beach"", and objects like ""cat"". The database was collected with extra care to ensure relevance to consumer interest and originality of video content without post-editing."
HMDB51,https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/,https://ieeexplore.ieee.org/abstract/document/6126543,2011,"Action localization, recognition","Facial actions, body movements, object and human interaction",6849 clips divided into 51 action categories,Both,"Movies, internet videos",F,Video,"Class labels, pose information, camera angle, video quality label","Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube."
MSVD,https://paperswithcode.com/dataset/msvd,https://aclanthology.org/P11-1020.pdf,2011,"Captioning, paraphrasing",Short actions,"85K English descriptions for 2,089 video clips",Unknown,Internet video clips,F,Video,Video captions,A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale.
UCF-101,https://www.crcv.ucf.edu/data/UCF101.php,https://www.crcv.ucf.edu/papers/UCF101_CRCV-TR-12-01.pdf,2012,Action Recognition,Daily life actions,"13,320 videos, 101 categories",Unknown,Internet videos,F,Video,"Class labels, semantic video groups",the largest dataset of human actions
Breakfast,http://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6909500,2014,Hierarchical action recognition,Cooking activities,"77 hours of video, 10 actions",Scripted,Multiview dataset content,F,Video,"Hierarchical class labels, temporal intervals","we collected a large dataset of daily cooking activities: The dataset includes a total of 52 participants, each performing a total of 10 cooking activities in multiple real-life kitchens, resulting in over 77 hours of video footage."
Sports-1M,https://cs.stanford.edu/people/karpathy/deepvideo/,https://cs.stanford.edu/people/karpathy/deepvideo/deepvideo_cvpr2014.pdf,2014,Action Recognition,Sports content,"1 million videos, 487 classes",Unknown,Internet videos,F,Video,Class labels,extensive empirical evaluation of CNNs on largescale video classification using a new dataset
VideoStory,https://ivi.fnwi.uva.nl/isis/mediamill/datasets/videostory.php,https://isis-data.science.uva.nl/cgmsnoek/pub/habibian-videostory-mm2014.pdf,2014,Captioning,Popular internet queries,"45,826 videos",Unknown,Internet videos,F,Video,Video captions,"This paper proposes a new video representation for fewexample event recognition and translation. Different from existing representations, which rely on either low-level features, or pre-specified attributes, we propose to learn an embedding from videos and their descriptions. In our embedding, which we call VideoStory, correlated term labels are combined if their combination improves the video classifier prediction."
ActivityNet,http://activity-net.org/index.html,https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.pdf,2015,"Action Recognition, Detection",Daily human activities,"200 classes, 100 videos per class",Unknown,Internet videos,F,Video,"Class labels, class temporal boundaries",current computer vision algorithms are still severely limited in terms of the variability and complexity of the actions that they can recognize. Our benchmark aims at covering a wide range of complexhuman activities that are of interest to people in their daily living
EventNet,http://eventnet.ee/,https://dl.acm.org/doi/abs/10.1145/2733373.2806221,2015,Retrieval,Events from WikiHow articles,"95,321 videos and 4,490 concepts",Unknown,Internet videos,F,Video,"Event classes, ""concept based representations of event videos""","Existing methods only focus on defining event-specific concepts for a small number of pre-defined events, but cannot handle novel unseen events. We perform a coarse-to-fine event discovery process and discover 500 events from WikiHow articles. Then we use each event name as query to search YouTube and discover event-specific concepts from the tags of returned videos. After an automatic filter process, we end up with 95,321 videos and 4,490 concepts."
MPII,https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/mpii-movie-description-dataset/,https://openaccess.thecvf.com/content_cvpr_2015/papers/Rohrbach_A_Dataset_for_2015_CVPR_paper.pdf,2015,Captioning,General movie actions conveying story information,"94 movies, 118,114 clips",Scripted,Movies,F,Video,"Audio descriptions, transcripts","In this work we propose a novel dataset which contains transcribed ADs, which are temporally aligned to full length HD movies. In addition we also collected the aligned movie scripts which have been used in prior work and compare the two different sources of descriptions."
MPII Cooking 2,https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/human-activity-recognition/mpii-cooking-2-dataset/,https://arxiv.org/pdf/1502.06648.pdf,2015,Hierarchical action recognition,People cooking,"87 activity classes, 273 videos",Scripted,Dataset content,F,Video,"Composite labels, time intervals, object labels, human/hand poses",the challenges of detecting fine-grained activities and understanding how they are combined into composite activities have been largely overlooked. In this work we approach both tasks and present a dataset which provides detailed annotations to address them
TVSum,https://github.com/yalesong/tvsum,https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Song_TVSum_Summarizing_Web_2015_CVPR_paper.pdf,2015,"Captioning, shot importance classification",10 categories from TRECVid,"50 videos, 1K annotations",Unknown,Internet videos,F,Video,Shot-level importance scores,"We present TVSum, an unsupervised video summarization framework that uses title-based image search results to find visually important shots. We observe that a video title is often carefully chosen to be maximally descriptive of its main topic, and hence images related to the title can serve as a proxy for important visual concepts of the main topic"
Charades,https://prior.allenai.org/projects/charades,http://ai2-website.s3.amazonaws.com/publications/hollywood-homes.pdf,2016,"Action Recognition, Captioning","daily indoors activities in (Verb, proposition, noun) form","157 classes, 9848 videos",Scripted,Dataset content,F,Video,"(Verb, proposition, noun) triplets, video descriptions, temporal intervals","computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes. While most of such scenes are not particularly exciting, they typically do not appear on YouTube, in movies or TV broadcasts. So how do we collect sufficiently many diverse but boring samples representing our lives? We propose a novel Hollywood in Homes approach to collect such data"
MovieQA,https://github.com/makarandtapaswi/MovieQA_benchmark,https://arxiv.org/abs/1512.02902,2016,QA,Movie 6WH topics,408 movies,Scripted,Movies,F,Video,"QA pairs, temporal intervals","a truly intelligent machine would ideally also infer high-level semantics underlying human actions such as motivation, intent and emotion, in order to react and, possibly, communicate appropriately. We introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 14,944 questions about 408 movies with high semantic diversity"
MSR-VTT,https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf,2016,Captioning,Daily Activities,10k videos,Unknown,Internet videos,F,Video,Video captions,"current computer vision algorithms are still severely limited in terms of the variability and complexity of the videos and their associated language that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on specific fine-grained domains with limited videos and simple descriptions. While researchers have provided several benchmark datasets for image captioning, we are not aware of any large-scale video description dataset with comprehensive categories yet diverse video content. In this paper we present MSR-VTT (standing for “MSRVideo to Text”) which is a new large-scale video benchmark for video understanding, especially the emerging task of translating video to text."
TGIF,http://raingo.github.io/TGIF-Release/,https://arxiv.org/pdf/1604.02748.pdf,2016,Captioning,Short events in GIFs,"100K videos, 120K descriptions",Both,Tumblr GIFs,F,Video,Natural language descriptions,"With the recent popularity of animated GIFs on social media, there is need for ways to index them with rich metadata. To advance research on animated GIF understanding, we collected a new dataset, Tumblr GIF (TGIF), with 100K animated GIFs from Tumblr and 120K natural language descriptions obtained via crowdsourcing"
Volleyball,https://github.com/mostafa-saad/deep-activity-rec,https://www2.cs.sfu.ca/~mori/research/papers/ibrahim-cvpr16.pdf,2016,Activity Recognition,Team sports actions,"55 videos, 17 classes",Natural,Sports footage,T,Video,Class labels,"the temporal dynamics of tthe whole activity can be inferred based on the dynamics of the individual people representing the activity. we also propose a new volleyball dataset that offers person detections, and both the person action label, as well as the group activity label"
VTW,https://arxiv.org/pdf/1608.07068v2.pdf,https://arxiv.org/pdf/1608.07068v2.pdf,2016,Captioning,Common events of human interest,"18,100 videos",Unknown,Internet videos highlighted by online communities,F,Video,Video titles,We propose a scalable approach to learn video-based question answering (QA): to answer a free-form natural language question about the contents of a video.
YouTube-8M,https://research.google.com/youtube8m/index.html,https://arxiv.org/pdf/1609.08675.pdf,2016,Video Classification,Knowledge graph entity-centric events,"8M videos, 1K classes",Unknown,Internet videos,F,Video,Class labels,"Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset"
ActivityNet-Captions,https://cs.stanford.edu/people/ranjaykrishna/densevid/,https://arxiv.org/abs/1705.00754,2017,Captioning,Daily human activities,"20K videos, 100K descriptions",Unknown,Internet videos,F,Video,"Video captions, temporal intervals","We introduce the task of dense-captioning events, which involves both detecting and describing events in a video"
DiDeMo,https://github.com/LisaAnne/TemporalLanguageRelease,https://arxiv.org/abs/1708.01641,2017,"Action Localization, Captioning",Personal daily content,10K videos,Unknown,Internet videos,F,Video,"Clip captions, temporal intervals","current video datasets do not include pairs of localized video segments and referring expressions, or text descriptions which uniquely identify a corresponding moment. Therefore, we collect the Distinct Describable Moments (DiDeMo) dataset which consists of over 10,000 unedited, personal videos in diverse visual settings with pairs of localized video segments and referring expressions"
Kinetics,https://www.deepmind.com/open-source/kinetics,https://arxiv.org/abs/1705.06950,2017,Action Recognition,Human-focused actions,"700 classes, 650k videos",Unknown,Internet videos,F,Video,Class labels,"We developed this dataset principally because there is a lack of such datasets for human action classification, and we believe that having one will facilitate research in this area"
MSR-VTT-QA,https://github.com/xudejing/video-question-answering,http://staff.ustc.edu.cn/~hexn/papers/mm17-videoQA.pdf,2017,QA,Daily activities,10K video clips and 243k question answer pairs,Unknown,Internet videos,F,Video,QA pairs,"Since VideoQA is a relatively new task, there is no available public dataset. [9] presents a method which can generate question answer pairs from descriptions automatically. We generate two datasets based on this method by converting video captions in existing datasets to question answering pairs (this dataset and MSVD-QA)."
MultiTHUMOS,https://ai.stanford.edu/~syyeung/everymoment.html,https://link.springer.com/article/10.1007/s11263-017-1013-y,2017,Hierarchical Action Recognition,Daily life actions,"400 videos, 65 classes",Unknown,Internet videos,F,Video,Frame-level action labels,"A comprehensive understanding of human activity in video requires labeling every frame according to the actions occurring, placing multiple labels densely over a video sequence. To study this problem we extend the existing THUMOS dataset and introduce MultiTHUMOS, a new dataset of dense labels over unconstrained internet videos"
ShanghaiTech Campus,https://svip-lab.github.io/dataset/campus_dataset.html,https://openaccess.thecvf.com/content_ICCV_2017/papers/Luo_A_Revisit_of_ICCV_2017_paper.pdf,2017,Anomaly detection,Anomalies in public areas,107 videos + 2 hrs subway footage,Unscripted,Multi-view surveilance videos,F,Video,Anomaly labels,We build a very large dataset which is even larger than the summation of all existing dataset for anomaly detection in terms of both the volume of data and the diversity of scenes
Something Something,https://developer.qualcomm.com/software/ai-datasets/something-something,https://arxiv.org/abs/1706.04261,2017,Action recognition,General actions,"220,847 labeled video clips",Scripted,Dataset content,F,Video,"""Caption-templates""","most labelled video datasets represent high-level concepts rather than detailed physical aspects about actions and scenes. In this work, we describe our ongoing collection of the “something-something” database of video prediction tasks whose solutions require a common sense understanding of the depicted situation."
THUMOS,https://www.crcv.ucf.edu/THUMOS14/,https://www.sciencedirect.com/science/article/pii/S1077314216301710,2017,Action Recognition,Daily life actions,"10K+ videos, 101 classes",Unknown,Internet videos,F,Video,"Class labels, secondary actions, temporal intervals",THUMOS challenge was introduced in 2013 to serve as a benchmark for action recognition.
VLOG,https://web.eecs.umich.edu/~fouhey//2017/VLOG/index.html,https://web.eecs.umich.edu/~fouhey//2017/VLOG/paper.pdf,2017,Video Classification,Daily life actions,"114K videos, 346 classes",Half-natural,Vlogs,F,Video,"Hand pose, object, location, nearby object, human pose, hand location","Most past efforts have gathered this data explicitly: starting with a laundry list of action labels, and then querying search engines for videos tagged with each label. In this work, we do the reverse and search implicitly: we start with a large collection of interaction-rich video data and then annotate and analyze it."
YouCook2,http://youcook2.eecs.umich.edu/,http://youcook2.eecs.umich.edu/static/YouCookII/youcookii_readme.pdf,2017,"Hierarchical localization, captioning",Cooking steps,"2K videos, 89 recipes",Natural,Internet cooking videos,F,Video,"Recipe description, temporal intervals, step descriptions","Learning from instructional video is a promising direction that may help ground the vision and language problem. To move toward this goal, we collect a largescale cooking video dataset, called YouCookII, with 2000 videos downloaded from YouTube."
AVA Actions,https://research.google.com/ava/,https://openaccess.thecvf.com/content_cvpr_2018/html/Gu_AVA_A_Video_CVPR_2018_paper.html,2018,"Action recognition, localization",Atomic actions in movies,80 atomic visual actions in 430 15-minute movie clips,Scripted,"Movies, TV",F,Video,"Action labels, bounding boxes, entity coreference","This paper introduces a video dataset of spatio-temporally localized Atomic Visual Actions (AVA). The AVA dataset densely annotates 80 atomic visual actions in 437 15-minute video clips, where actions are localized in space and time, resulting in 1.59M action labels with multiple labels per person occurring frequently."
Charades-Ego,https://prior.allenai.org/projects/charades-ego,https://arxiv.org/pdf/1804.09627.pdf,2018,Action Recognition,daily indoors activities,"7860 videos, 157 classes",Scripted,First and third person dataset content,F,Video,Charades action labels,"Several theories in cognitive neuroscience suggest that when people interact with the world, or simulate interactions, they do so from a first-person egocentric perspective, and seamlessly transfer knowledge between third-person (observer) and first-person (actor). Despite this, learning such models for human action recognition has not been achievable due to the lack of data. This paper takes a step in this direction, with the introduction of Charades-Ego, a large-scale dataset of paired first-person and third-person videos"
Diving48,http://www.svcl.ucsd.edu/projects/resound/dataset.html,https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Yingwei_Li_RESOUND_Towards_Action_ECCV_2018_paper.pdf,2018,Hierarchical Action Recognition,Hierarchical diving actions,"18K+ videos, 48 classes",Natural,Diving videos,F,Video,Hierarchical class labels,"A procedure, RESOUND, is proposed to quantify and minimize representation bias. Its application to the problem of action recognition shows that current datasets are biased towards static representations (objects, scenes and people). An implicit RESOUND procedure is used to guide the creation of a new dataset, Diving48, of over 18,000 video clips of competitive diving actions, spanning 48 fine-grained dive classes"
Epic Kitchens,https://epic-kitchens.github.io/2021,https://openaccess.thecvf.com/content_ECCV_2018/papers/Dima_Damen_Scaling_Egocentric_Vision_ECCV_2018_paper.pdf,2018,Hierarchical Action Recognition,Cooking videos,"97 verbs, 100 hours of video",Scripted,First-person dataset content,F,"Video, narration","Transcripts, temporal intervals, active object bounding boxes","First-person vision is gaining interest as it offers a unique viewpoint on people’s interaction with objects, their attention, and even intention. However, progress in this challenging domain has been relatively slow due to the lack of sufficiently large datasets"
FCVid,https://fvl.fudan.edu.cn/dataset/fcvid/list.htm,https://ieeexplore.ieee.org/document/7857793,2018,Classification,Daily life,"91,223 Web videos annotated manually according to 239 categories",Unknown,Internet videos,F,Video,Class labels,"To stimulate future research on large scale video categorization, we collect and release a new benchmark dataset, called FCVID, which contains 91,223 Internet videos and 239 manually annotated categories."
Moments in Time,http://moments.csail.mit.edu/,http://moments.csail.mit.edu/TPAMI.2019.2901464.pdf,2018,Action Recognition,Short events (3s) from verb databases,"339 verbs, 1K+ videos per verb",Unknown,Internet videos,F,Video,Class labels,"The Moments in Time dataset, designed to have a large coverage and diversity of events in both visual and auditory modalities, can serve as a new challenge to develop models that scale to the level of complexity and abstract reasoning that a human processes on a daily basis."
MovieGraphs,http://moviegraphs.cs.toronto.edu/,https://arxiv.org/pdf/1712.06761.pdf,2018,Scene Parsing,Movie social interactions,51 movies,Scripted,Movies,F,Video,"Video description, scene graph","There is growing interest in artificial intelligence to build socially intelligent robots. This requires machines to have the ability to “read” people’s emotions, motivations, and other factors that affect behavior. Towards this goal, we introduce a novel dataset called MovieGraphs which provides detailed, graph-based annotations of social situations depicted in movie clips."
TEMPO,https://github.com/LisaAnne/TemporalLanguageRelease,https://arxiv.org/pdf/1809.01337.pdf,2018,"Localization, captioning",Personal daily content,10K videos,Unknown,Internet videos,F,Video,"Templated captions, natural language captions, temporal intervals","To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset"
V3C,https://sites.google.com/view/viral2019/home/supported-datasets,https://arxiv.org/pdf/1810.04401.pdf,2018,Retrieval,"Art, photography, comedy, fashion, food, instructionals, music, narrative, journalism","7475 Vimeo videos segmented into 1,082,657 short video segments",Both,Internet videos,F,Video,"Semantic temporal segments, key frames, metadata, video captions","existing video datasets used for research and experimentation are either not large enough to represent current collections or do not reflect the properties of video commonly found on the Internet in terms of content, length, or resolution. In this paper, we introduce the Vimeo Creative Commons Collection, in short V3C, a collection of 28’450 videos (with overall length of about 3’800 hours) published under creative commons license on Vimeo"
ActivityNet-Entities,https://github.com/facebookresearch/ActivityNet-Entities,https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Grounded_Video_Description_CVPR_2019_paper.pdf,2019,Captioning,Daily human activities,"15K videos, 52K event segments",Unknown,Internet videos,F,Video,ActivityNet-Captions annotations + entity bounding boxes,"Video description is one of the most challenging problems in vision and language understanding due to the large variability both on the video and language side. Models, hence, typically shortcut the difficulty in recognition and generate plausible sentences that are based on priors but are not necessarily grounded in the video. In this work, we explicitly link the sentence to the evidence in the video by annotating each noun phrase in a sentence with the corresponding bounding box in one of the frames of a video. Our dataset, ActivityNet-Entities, augments the challenging ActivityNet Captions dataset with 158k bounding box annotations, each grounding a noun phrase."
COIN,https://coin-dataset.github.io/,https://arxiv.org/pdf/1903.02874.pdf,2019,Hierarchical Action Recognition,"nursing & caring, vehicles, leisure & performance, gadgets, electric appliances, household items, science & craft, plants & fruits, snacks & drinks dishes, sports, and housework","11,827 videos, 180 classes",Scripted,Instructional videos,F,Video,"Class labels, temporal intervals","However, most existing datasets for instructional video analysis have the limitations in diversity and scale, which makes them far from many real-world applications where more diverse activities occur. Moreover, it still remains a great challenge to organize and harness such data. To address these problems, we introduce a large-scale dataset called “COIN”"
CrossTask,https://github.com/DmZhukov/CrossTask,https://arxiv.org/pdf/1903.08225.pdf,2019,Task ordering,Instruction steps,"83 tasks, 4.7K videos",Scripted,Instructional videos,F,Video,"Class labels, temporal intervals","weakly supervised learning may be easier if a model shares components while learning different steps: “pour egg” should be trained jointly with other tasks involving “pour” and “egg”. We formalize this in a component model for recognizing steps and a weakly supervised learning framework that can learn this model under temporal constraints from narration and the list of steps. Past data does not permit systematic studying of sharing and so we also gather a new dataset, CrossTask"
FIVR-200K,https://github.com/MKLab-ITI/FIVR-200K,https://arxiv.org/pdf/1809.04094v2.pdf,2019,Retrieval,Armed conflicts and disasters,"225,960 videos associated with 4,687 Wikipedia events",Both,Internet news videos,T,"Video, text articles",Scene labels,"This paper introduces the problem of Fine-grained Incident Video Retrieval (FIVR). Given a query video, the objective is to retrieve all associated videos, considering several types of associations that range from duplicate videos to videos from the same incident. To address the benchmarking needs of all such tasks, we construct and present a large-scale annotated video dataset, which we call FIVR-200K, and it comprises 225,960 videos. "
HowTo100M,https://www.di.ens.fr/willow/research/howto100m/,https://arxiv.org/pdf/1906.03327.pdf,2019,Multiple tasks,Instructional content,"136M clips from 1.2M YT videos, 23K activities",Scripted,Instructional videos,F,Video,"Transcripts, temporal intervals","Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations."
MMAct,https://mmact19.github.io/2019/,https://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_MMAct_A_Large-Scale_Dataset_for_Cross_Modal_Human_Action_Understanding_ICCV_2019_paper.pdf,2019,Action Recognition,"Daily actions, ""abnormal"" actions, desk work","1900+ videos, 37 classes",Scripted,"4 surveillance views + egocentric view, dataset content",F,"RGB, Keypoints, Acceleration, Gyroscope, Orientation, Wi-Fi, Pressure.",Class labels,"body-worn sensors or passive sensing can avoid the failure of action understanding in vision related challenges, e.g. occlusion and appearance variation. However, a standard large-scale dataset does not exist, in which different types of modalities across vision and sensors are integrated. To address the disadvantage of vision-based modalities and push towards multi/cross modal action understanding, this paper introduces a new large-scale dataset"
MTL-AQA,http://rtis.oit.unlv.edu/datasets/,https://openaccess.thecvf.com/content_CVPR_2019/html/Parmar_What_and_How_Well_You_Performed_A_Multitask_Learning_Approach_CVPR_2019_paper.html,2019,"Action quality assessment, action recognition, captioning",Diving actions from 16 diving events,1412 diving samples,Natural,TV diving competition coverage,F,Video,"Transcripts, fine-grained dive classification labels","In this paper, we propose to learn spatio-temporal features that explain three related tasks - fine-grained action recognition, commentary generation, and estimating the AQA score. A new multitask-AQA dataset, the largest to date, comprising of 1412 diving samples was collected to evaluate our approach"
Social-IQ,https://cmu-multicomp-lab.github.io/social-iq-2.0/,https://openaccess.thecvf.com/content_CVPR_2019/papers/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.pdf,2019,QA,Social interactions,"1, 250 videos, 7, 500",Unknown,Internet videos,F,Video,"Transcripts, QA pairs","In this paper, we introduce the Social-IQ, an unconstrained benchmark specifically designed to train and evaluate socially intelligent technologies."
WebVid-2M,https://github.com/m-bain/frozen-in-time,https://arxiv.org/pdf/2104.00650.pdf,2019,Captioning,Unknown,2.6M video clips,Unknown,Internet videos,F,Video,Video captions,"Our objective in this work is video-text retrieval – in particular a joint embedding that enables efficient text-to-video retrieval. The challenges in this area include the design of the visual architecture and the nature of the training data, in that the available large scale video-text training datasets, such as HowTo100M, are noisy and hence competitive performance is achieved only at scale through large amounts of compute. We also provide a new video-text pretraining dataset WebVid-2M, comprised of over two million videos with weak captions scraped from the internet"
CondensedMovies,https://www.robots.ox.ac.uk/~vgg/data/condensed-movies/,https://arxiv.org/abs/2005.04208,2020,Retrieval,General movie actions,"3K movies, 33,976 clips",Scripted,Movies,F,Video,"High level semantic scene description, character face-tracks, movie metadata","Our objective in this work is long range understanding of the narrative structure of movies. Instead of considering the entire movie, we propose to learn from the ‘key scenes’ of the movie, providing a condensed look at the full storyline. To this end, we make the following three contributions: (i) We create the Condensed Movies Dataset (CMD) consisting of the key scenes from over 3K movies: each key scene is accompanied by a high level semantic description of the scene, character face-tracks, and metadata about the movie"
DramaQA,https://dramaqa.snu.ac.kr/,https://arxiv.org/abs/2005.03356,2020,QA,Character-centric story events,"17,983 QA pairs from 23,928 various length video clips",Scripted,TV drama,F,Video,"QA pairs, character bounding boxes, emotion labels, entity coreference resolutions","we propose a novel video question answering (Video QA) task, DramaQA, for a comprehensive understanding of the video story. The DramaQA focuses on two perspectives: 1) Hierarchical QAs as an evaluation metric based on the cognitive developmental stages of human intelligence. 2) Character-centered video annotations to model local coherence of the story."
FineGym,https://sdolivia.github.io/FineGym/,https://arxiv.org/pdf/2004.06704.pdf,2020,Hierarchical Action Recognition,Hierarchical gymnastics actions,"10 event categories, 303 videos",Natural,Gymnastics competition recordings,F,Video,"Hierarchical class labels, temporal intervals","On public benchmarks, current action recognition techniques have achieved great success. However, when used in real-world applications, e.g. sport analysis, which requires the capability of parsing an activity into phases and differentiating between subtly different actions, their performances remain far from being satisfactory. To take action recognition to a new level, we develop FineGym1 , a new dataset built on top of gymnastic videos"
HLVU,https://github.com/usnistgov/HLVU,https://arxiv.org/pdf/2005.00463.pdf,2020,Scene Parsing,General movie actions,10 movies,Scripted,Movies,F,Video,"Character/location relationships, QA pairs","In this paper we propose a new evaluation challenge and direction in the area of High-level Video Understanding. The challenge we are proposing is designed to test automatic video analysis and understanding, and how accurately systems can comprehend a movie in terms of actors, entities, events and their relationship to each other. A pilot High-Level Video Understanding (HLVU) dataset of open source movies were collected for human assessors to build a knowledge graph representing each of them"
HowToVQA69M,https://antoyang.github.io/just-ask.html,https://arxiv.org/abs/2012.00451,2020,QA,Instructional content,69M video-questionanswer triplets,Scripted,Instructional videos,F,Video,QA pairs,"We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video narrations. Given narrated videos, we then automatically generate the HowToVQA69M dataset with 69M video-questionanswer triplets."
HVU,https://holistic-video-understanding.github.io/,https://pages.iai.uni-bonn.de/gall_juergen/download/HVU_eccv20.pdf,2020,Video Classification,Daily human activities,"577K videos, 3K classes",Unknown,Internet videos,F,Video,"Scene, object, action, event, attribute, and concept labels","However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a largescale “Holistic Video Understanding Dataset” (HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene."
LEMMA,https://sites.google.com/view/lemma-activity,https://arxiv.org/pdf/2007.15781.pdf,2020,Hierarchical action Recognition,"Multi-agent, ""complex"" human activities","324 activities, 10.1 hours of footage ",Scripted,First and third person dataset content,F,"Video, depth, skeletons","Human bounding boxes, verb/noun labels, temporal intervals, class label","However, a few imperative components of daily human activities are largely missed in prior literature, including the goal-directed actions, concurrent multi-tasks, and collaborations among multi-agents. We introduce the LEMMA dataset to provide a single home to address these missing dimensions"
MovieNet,https://movienet.github.io/,https://arxiv.org/pdf/2007.10937.pdf,2020,Multiple tasks,General movie actions,1.1K movies,Scripted,Movies,F,"Movie, subtitles, IMDB metadata","Cinematic style, character bounding boxes, scene boundaries, action/place tags, video captions","how to understand a story-based long video with artistic styles, e.g. movie, remains challenging. In this paper, we introduce MovieNet – a holistic dataset for movie understanding"
NBA,https://ruiyan1995.github.io/SAM.html,https://arxiv.org/pdf/2007.09470.pdf,2020,Activity Recognition,Group basketball activities,"9,172 videos, 9 classes",Natural,Basketball footage,F,Video,Class labels,"This paper presents a new task named weakly-supervised group activity recognition (GAR) which differs from conventional GAR tasks in that only video-level labels are available, yet the important persons within each frame are not provided even in the training data. This eases us to collect and annotate a large-scale NBA dataset and thus raise new challenges to GAR."
TVQA+,https://tvqa.cs.unc.edu/,https://arxiv.org/abs/1904.11574,2020,QA,Events involving 6W1H,152.5K QA pairs from 21.8K video clips from 6 TV shows,Unknown,TV shows,F,Video,"Multiple choice questions, temporal ground truth intervals, relevant entity bounding boxes and labels, transcripts, transcript temporal intervals","We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos."
TVR/TVC,https://tvr.cs.unc.edu/,https://arxiv.org/pdf/2001.09099.pdf,2020,"Retrieval, Captioning","Events pertaining to TV visual content, TV audio content, and both","108,965 queries on 21,793 videos from 6 TV shows",Scripted,TV shows,F,Video,"Transcript, queries, query type labels, temporal intervals","We introduce TV show Retrieval (TVR), a new multimodal retrieval dataset. TVR requires systems to understand both videos and their associated subtitle (dialogue) texts, making it more realistic."
VaTeX,https://eric-xw.github.io/vatex-website/download.html,https://arxiv.org/pdf/1904.03493.pdf,2020,Captioning,Human-focused actions,35K videos,Unknown,Internet videos,F,Video,Multilingual video captions,"Compared to the widely-used MSRVTT dataset [66], VATEX is multilingual, larger, linguistically complex, and more diverse in terms of both video and natural language descriptions. We also introduce two tasks for video-and-language research based on VATEX"
VIOLIN,https://github.com/jimmy646/violin,https://arxiv.org/pdf/2003.11618.pdf,2020,Inference,Social/complex narrative events,"95,322 video-hypothesis pairs from 15,887 video clips",Scripted,"TV shows, movie clips",F,Video,Video entailment pairs,"We introduce a new task, Video-and-Language Inference, for joint multimodal understanding of video and text. Given a video clip with aligned subtitles as premise, paired with a natural language hypothesis based on the video content, a model needs to infer whether the hypothesis is entailed or contradicted by the given video clip"
VITT,https://github.com/google-research-datasets/Video-Timeline-Tags-ViTT,https://arxiv.org/pdf/2011.11760.pdf,2020,Captioning,"Instructional steps, especially cooking related","8,169 videos",Scripted,YouTube 8M subset,F,Video,"Video captions, temporal intervals","we construct and release a new dense video captioning dataset, Video Timeline Tags (ViTT), featuring a variety of instructional videos together with time-stamped annotations."
VLEP,https://github.com/jayleicn/VideoLanguageFuturePred,https://arxiv.org/pdf/2010.07999.pdf,2020,Prediction,Commonsense future narrative events,28K+ videos,Both,TV shows and vlogs,F,Video,"Future event predictions, rationales","Given a video with aligned dialogue, people can often infer what is more likely to happen next.  In this work, we explore whether AI models are able to learn to make such multimodal commonsense nextevent predictions. To support research in this direction, we collect a new dataset, named Video-and-Language Event Prediction (VLEP), with 28,726 future event prediction examples (along with their rationales) from 10,234 diverse TV Show and YouTube Lifestyle Vlog video clips."
AGQA,https://cs.stanford.edu/people/ranjaykrishna/agqa/,https://arxiv.org/pdf/2103.16002.pdf,2021,QA,daily indoors activities,192M unbalanced question answer pairs for 9.6K videos,Scripted,Dataset content,F,Video,"Spatio-temporal scene graphs, QA pairs","We present Action Genome Question Answering (AGQA), a new benchmark for compositional spatio-temporal reasoning. AGQA contains 192M unbalanced question answer pairs for 9.6K videos. We also provide a balanced subset of 3.9M question answer pairs, 3 orders of magnitude larger than existing benchmarks, that minimizes bias by balancing the answer distributions and types of question structures"
Ego4D,https://ego4d-data.org/,https://arxiv.org/abs/2110.07058,2021,Multiple tasks,Daily life content,"931 unique camera wearers, 3,670 hours of video, 14 scenarios make up 70% of data (long-tailed)",Natural,Dataset content,F,"Video, 3D meshes of the environment, eye gaze, stereo","Narration, QA pairs, temporal annotations, action labels, bounding boxes, speaker labels, semantic action/role labels","We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of dailylife activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries."
HACS,http://hacs.csail.mit.edu/,https://arxiv.org/abs/1712.09374,2021,Action classification and localization,Daily human activities,"1.55M clip annotations, 504K videos, 200 action classes",Unknown,Internet videos,F,Video,"Class labels, temporal intervals",This paper presents a new large-scale dataset for recognition and temporal localization of human actions collected from Web videos. We refer to it as HACS (Human Action Clips and Segments).
Homage,https://homeactiongenome.org/,https://arxiv.org/pdf/2105.05226.pdf,2021,Hierarchical Action Recognition,indoor daily activities,"70 activities, 453 actions, 30 hours of video",Scripted,Multi-view dataset content,F,Video,"Video-level activity labels, temporally localized atomic activity labels, spatio-temporal scene-graph labels","Existing research on action recognition treats activities as monolithic events occurring in videos. Recently, the benefits of formulating actions as a combination of atomicactions have shown promise in improving action understanding with the emergence of datasets containing such annotations, allowing us to learn representations capturing this information. However, there remains a lack of studies that extend action composition and leverage multiple viewpoints and multiple modalities of data for representation learning. To promote research in this direction, we introduce Home Action Genome (HOMAGE): a multi-view action dataset with multiple modalities and view-points supplemented with hierarchical activity and atomic action labels together with dense scene composition labels"
MOMA,https://moma.stanford.edu/#/publication,https://proceedings.neurips.cc/paper/2021/file/95688ba636a4720a85b3634acfec8cdd-Paper.pdf,2021,Scene Parsing,Events and sub events from 17 hierarchical activity classes,"17 activity categories, 67 sub-activities, 52 atomic categories, 373 videos",Unknown,Internet videos,F,Video,"Four-level hierarchy action labels, entity labels, entity relationship labels","Involving multiple entities (actors and objects), we argue that traditional pair-wise relationships, often used in scene or action graphs, do not appropriately represent the dynamics between them. Hence, we introduce Action Hypergraph, a new representation of spatial-temporal graphs containing hyperedges (i.e., edges with higher-order relationships)."
MTVR,https://github.com/jayleicn/mTVRetrieval,https://arxiv.org/pdf/2108.00061.pdf,2021,Retrieval,"Events pertaining to TV visual content, TV audio content, and both",21.8K videos,Scripted,TV shows,F,Video,TVR annotations + multilingual queries and transcripts,"Compared to existing moment retrieval datasets, MTVR is multilingual, larger, and comes with diverse annotations."
Multi-HowTo100M,https://github.com/berniebear/Multi-HT100M,https://arxiv.org/pdf/2103.08849.pdf,2021,Retrieval,Instructional content,1.1M videos,Scripted,Instructional videos,F,Video,"Multilingual transcripts, temporal intervals","Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (MultiHowTo100M) for pre-training."
Multi-MomentsInTime,http://moments.csail.mit.edu/,https://arxiv.org/pdf/1911.00232.pdf,2021,"Action recognition, captioning",Short events (3s) from verb databases,"2.01 million labels for 1.02 million videos, 292 action classes",Unknown,Internet videos,F,Video,Class labels,"However, most large-scale datasets built to train models for action recognition in video only provide a single label per video. Consequently, models can be incorrectly penalized for classifying actions that exist in the videos but are not explicitly labeled and do not learn the full spectrum of information present in each video in training. Towards this goal, we present the Multi-Moments in Time dataset (M-MiT) which includes over two million action labels for over one million three second videos. This multi-label dataset introduces novel challenges on how to train and analyze models for multi-action detection."
MUSES,https://songbai.site/muses/,https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Multi-Shot_Temporal_Event_Localization_A_Benchmark_CVPR_2021_paper.html,2021,Action recognition,25 categories involving social relations and daily life,"31,477 event instances for a total of 716 video hours",Scripted,Dramas,F,Video,"Class labels, temporal intervals","In this paper, we propose a new and challenging task called multi-shot temporal event localization, and accordingly, collect a large-scale dataset called MUlti-Shot EventS (MUSES). MUSES has 31,477 event instances for a total of 716 video hours"
NEXT-QA,https://doc-doc.github.io/docs/nextqa.html,https://arxiv.org/pdf/2105.08276.pdf,2021,QA,Daily life events,5440 videos and 52K QA pairs,Natural,Internet videos,F,Video,Multiple choice questions,"We introduce NExT-QA, a rigorously designed video question answering (VideoQA) benchmark to advance video understanding from describing to explaining the temporal actions"
QuerYD,https://www.robots.ox.ac.uk/~vgg/data/queryd/,https://arxiv.org/pdf/2011.11071.pdf,2021,"Retrieval, localization","Varied real + staged content including lectures, cartoons, and daily life","31K descriptions, 207 hrs of video",Both,Internet videos,T,Video,"Audio description of video content, temporal intervals","We introduce QuerYD, a new large-scale dataset for retrieval and event localisation in video. A unique feature of our dataset is the availability of two audio tracks for each video: the original audio, and a high-quality spoken description of the visual content"
RUDDER,https://rudder-2021.github.io/,https://arxiv.org/pdf/2103.05457.pdf,2021,Retrieval,Instructional steps,"492(to be updated) videos, with an average length of 80 seconds and around 7 sentences describing every video",Scripted,Instructional videos for making toys,F,Video,Multilingual transcripts,"we introduce Rudder - a multilingual video-text retrieval dataset that includes audio and textual captions in Marathi, Hindi, Tamil, Kannada, Malayalam and Telugu. Furthermore, we propose to compensate for data scarcity by using domain knowledge to augment supervision."
Spoken Moments,http://moments.csail.mit.edu/spoken.html,https://openaccess.thecvf.com/content/CVPR2021/html/Monfort_Spoken_Moments_Learning_Joint_Audio-Visual_Representations_From_Video_Descriptions_CVPR_2021_paper.html,2021,Captioning,Short events (3s) from verb databases,500K videos,Unknown,Internet videos,F,Video,Spoken video captions,"Existing caption datasets for video understanding are either small in scale or restricted to a specific domain. To address this, we present the Spoken Moments (S-MiT) dataset of 500k spoken captions each attributed to a unique short video depicting a broad range of different events. We collect our descriptions using audio recordings to ensure that they remain as natural and concise as possible while allowing us to scale the size of a large classification dataset"
STAR,https://bobbywu.com/STAR/,https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/5ef059938ba799aaa845e1c2e8a762bd-Paper-round2.pdf,2021,QA,Human actions and interactions,"22K videos, 60K questions, 140K situation graphs,  111 action predicates, 28 objects, and 24 relationships",Natural,"""Human activity videos""",F,Video,"Video hypergraphs, QA pairs",". This paper introduces a new benchmark that evaluates the situated reasoning ability via situation abstraction and logic-grounded question answering for real-world videos, called Situated Reasoning in Real-World Videos (STAR)."
VALUE,https://value-benchmark.github.io/,https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/a97da629b098b75c294dffdc3e463904-Paper-round1.pdf,2021,Multiple tasks,Wide range of events from multiple existing datasets,"TVR [35], How2R [36], TVQA [32], How2QA [36], VIOLIN [40], VLEP [34] and TVC, and youCook2, and VATEX",Both,Large collection of other datasets,F,Video,Multiple annotation types dependent on sub-dataset,"a truly useful VidL system is expected to be easily generalizable to diverse tasks, domains, and datasets. To facilitate the evaluation of such systems, we introduce Video-And-Language Understanding Evaluation (VALUE) benchmark, an assemblage of 11 VidL datasets over 3 popular tasks: (i) text-to-video retrieval; (ii) video question answering; and (iii) video captioning."
VideoLT,https://videolt.github.io/,https://arxiv.org/pdf/2105.02668.pdf,2021,Video Classification,"Animal, Art, Beauty and Fashion, Cooking, DIY, Education and Tech, Everyday life, Housework, Leisure and Tricks, Music, Nature, Sports and Travel (long tailed event distribution)","1,004 classes, 256,218 videos",Unknown,Internet videos,F,Video,Class labels,"Label distributions in real-world are oftentimes longtailed and imbalanced, resulting in biased models towards dominant labels. While long-tailed recognition has been extensively studied for image classification tasks, limited effort has been made for the video domain"
VidSitu,https://vidsitu.org/,https://arxiv.org/abs/2104.00990,2021,Scene Parsing,2s events from PropBank,"3K movies, 29K clips, 145K events",Scripted,Movies,F,Video,"Event labels, semantic role labels, event relationships, temporal intervals, entity coreferences","We propose a new framework for understanding and representing related salient events in a video using visual semantic role labeling. We represent videos as a set of related events, wherein each event consists of a verb and multiple entities that fulfill various roles relevant to that event"
VM2E2,_,https://blender.cs.illinois.edu/paper/m2e2video.pdf,2021,Scene Parsing,LDC event templates,860 video-article pairs,Both,News footage,T,"Video, text articles","Event types, temporal intervals, event coreference links, argument roles","we propose the first approach to jointly extract events from video and text articles. We also construct and will publicly release a new benchmark of video-article pairs, consisting of 860 video-article pairs with extensive annotations for evaluating methods on this task. "
3MASSIV,https://github.com/ShareChatAI/3MASSIV,https://arxiv.org/pdf/2203.14456.pdf,2022,Video Classification,Short social media video events,50k short videos (20 seconds average duration) and 100K unlabeled videos in 11 different languages,Both,Social media videos,F,Video,"Class labels, emotion labels, audio type labels, video type labels, language labels","We present 3MASSIV, a multilingual, multimodal and multi-aspect, expertly-annotated dataset of diverse short videos extracted from short-video social media platform - Moj."
Animal Kingdom,https://sutdcv.github.io/Animal-Kingdom/,https://openaccess.thecvf.com/content/CVPR2022/html/Ng_Animal_Kingdom_A_Large_and_Diverse_Dataset_for_Animal_Behavior_CVPR_2022_paper.html,2022,"Action recognition, localization",Diverse multi-label animal activities,30K video sequences,Unknown,Animal behavior,F,Video,"Action and behavior labels, temporal intervals, pose estimation frames","existing animal behavior datasets have limitations in multiple aspects, including limited numbers of animal classes, data samples and provided tasks, and also limited variations in environmental conditions and viewpoints. To address these limitations, we create a large and diverse dataset, Animal Kingdom, that provides multiple annotated tasks to enable a more thorough understanding of natural animal behaviors."
Assembly101,https://assembly-101.github.io/,https://openaccess.thecvf.com/content/CVPR2022/html/Sener_Assembly101_A_Large-Scale_Multi-View_Video_Dataset_for_Understanding_Procedural_Activities_CVPR_2022_paper.html,2022,"Action recognition, localization",Fine- and coarse-grained assembly actions,"4321 videos of people assembling and disassembling 101 ""take-apart"" toy vehicles",Scripted,Dataset content,F,Video,"Coarse- and fine-grained action segments, 3D hand poses","Assembly101 is a new procedural activity dataset featuring 4321 videos of people assembling and disassembling 101 “take-apart” toy vehicles. Assembly101 is the first multi-view action dataset, with simultaneous static (8) and egocentric (4) recordings."
HD-VILA-100M,https://github.com/microsoft/XPretrain/blob/main/hd-vila-100m/README.md,https://arxiv.org/pdf/2111.10337.pdf,2022,Multiple tasks,Random internet content,"3.3M videos, 15 categories",Unknown,Internet videos,F,Video,Video captions,"we propose a novel High-resolution and Diversified VIdeo-LAnguage pre-training model (HDVILA) for many visual tasks. In particular, we collect a large dataset with two distinct properties: 1) the first highresolution dataset including 371.5k hours of 720p videos, and 2) the most diversified dataset covering 15 popular YouTube categories"
How-2,https://github.com/srvk/how2-dataset,https://ieeexplore.ieee.org/document/9747320,2022,Captioning,Instructional content,"80,000 instructional videos",Scripted,Instructional videos,F,Video,"Transcripts, translations, video summaries","In this work, we introduce a single model optimized end-to-end for speech summarization. We apply the restricted self-attention technique from text-based models to speech models to address the memory and compute constraints. We demonstrate that the proposed model learns to directly summarize speech for the How-2 corpus of instructional videos."
MAD,https://github.com/Soldelli/MAD,https://arxiv.org/pdf/2112.00431.pdf,2022,"Localization, captioning",General movie actions,"650 movies, 1.2K clips",Scripted,Movies,F,Video,"Audio descriptions, temporal intervals","we present MAD (Movie Audio Descriptions), a novel benchmark that departs from the paradigm of augmenting existing video datasets with text annotations and focuses on crawling and aligning available audio descriptions of mainstream movies."
VideoCC3M,https://github.com/google-research-datasets/videoCC-data?tab=readme-ov-file,https://arxiv.org/pdf/2204.00679.pdf,2022,Retrieval,General internet video categories,"10.3M clip-text pairs, 6.3M video clips, 970K unique captions",Unknown,Internet videos,F,Video,Video captions,"we propose a new video mining pipeline which involves transferring captions from image captioning datasets to video clips with no additional manual effort. Using this pipeline, we create a new largescale, weakly labelled audio-video captioning dataset consisting of millions of paired clips and captions."
AIDA,https://catalog.ldc.upenn.edu/LDC2023T11,_,2023,Scene Parsing,Current events,291 videos,Both,Internet news videos,T,"Video, text articles, images",Event-relevant labels: Event slots and links,"To develop a multi-hypothesis semantic engine to generate explicit alternative interpretations of events, situations and trends from a variety of unstructured sources."
ChinaOpen,https://ruc-aimc-lab.github.io/ChinaOpen/,https://arxiv.org/pdf/2305.05880.pdf,2023,Captioning/Retrieval,Random internet content,50k videos,Unknown,Internet videos,F,Video,"Multilingual video titles, video tags","While the state-of-the-art multimodal learning networks have shown impressive performance in automated video annotation and cross-modal video retrieval, their training and evaluation are primarily conducted on YouTube videos with English text. Their effectiveness on Chinese data remains to be verified. In order to support multimodal learning in the new context, we construct ChinaOpen-50k, a webly annotated training set of 50k Bilibili videos associated with user-generated titles and tags"
EgoSchema,https://egoschema.github.io/,https://arxiv.org/abs/2308.09126,2023,QA,Daily life,"5000 human curated multiple choice question answer pairs, spanning over 250 hours of real video data",Natural,Multi-perspective dataset content,F,Video,Ego4D annotations + QA pairs,"We introduce EgoSchema, a very long-form video question-answering dataset, and benchmark to evaluate long video understanding capabilities of modern vision and language systems."
EmoMV,https://zenodo.org/records/7011072,https://www.sciencedirect.com/science/article/pii/S1566253522001725,2023,Emotion recognition,Emotions,4914 music video segments,Scripted,Music videos,F,Video,Class labels,"Studies in affective audio–visual correspondence learning require ground-truth data to train, validate, and test models. The number of available datasets together with benchmarks, however, is still limited. In this paper, we create a collection of three datasets (called EmoMV) for affective correspondence learning between music and video modalities."
IntentQA,https://github.com/JoseponLee/IntentQA,https://openaccess.thecvf.com/content/ICCV2023/papers/Li_IntentQA_Context-aware_Video_Intent_Reasoning_ICCV_2023_paper.pdf,2023,QA,Daily life events,"4, 303 videos, 16, 297 QA pairs, 624 actions, 193 verbs, 162 action IDs",Natural,Internet videos,F,Video,"QA pairs, action labels","In this paper, we propose a novel task IntentQA, a special VideoQA task focusing on video intent reasoning, which has become increasingly important for AI with its advantages in equipping AI agents with the capability of reasoning beyond mere recognition in daily tasks."
InternVid,https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid,https://arxiv.org/abs/2307.06942,2023,Multiple tasks,General internet video categories,18 million video clips,Unknown,Internet videos,F,Video,"Coarse- and fine-grained captions, temporal intervals","This paper introduces InternVid, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. The InternVid dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale."
Marine Video Kit,https://mvk.hkustvgd.com/,https://link.springer.com/chapter/10.1007/978-3-031-27077-2_42,2023,Retrieval,Basic marine organism events,1374 videos,Natural,Underwater footage,F,Video,"Video metadata, video captions","Effective analysis of unusual domain specific video collections represents an important practical problem, where state-of-the-art general purpose models still face limitations. we focus on single-shot videos taken from moving cameras in underwater environments, which constitute a nontrivial challenge for research purposes. The first shard of a new Marine Video Kit dataset is presented to serve for video retrieval and other computer vision challenges."
Multi-YouCook2,https://github.com/roudimit/c2kd,https://arxiv.org/pdf/2210.03625.pdf,2023,"Hierarchical localization, captioning",Cooking steps,12K+ videos,Natural,Internet cooking videos,F,Video,YouCook2 annotations + multilingual captions,"We introduce a new multilingual video dataset, Multi-YouCook2, by translating the English captions in the YouCook2 video dataset to 8 other languages"
MultiHiEve,_,https://arxiv.org/pdf/2206.07207.pdf,2023,Scene Parsing,Current event-related sub-events,100K videos,Both,Internet news videos,T,"Video, text article","Temporal intervals, event labels, event relationships","In this paper, we propose the task of extracting event hierarchies from multimodal (video and text) data to capture how the same event manifests itself in different modalities at different semantic levels. This reveals the structure of events and is critical to understanding them. To support research on this task, we introduce the Multimodal Hierarchical Events (Multi-HiEve) dataset"
MultiVENT,https://katesanders9.github.io/multiVENT-dataset/,https://arxiv.org/abs/2307.03153,2023,Retrieval,Current events,"2,396 videos and corresponding text descriptions covering 260 current events",Both,News footage,T,"Video, text articles","Multilingual video captions, class labels","existing news video datasets focus on traditional news broadcasts produced for English-speaking audiences. We address this limitation by constructing MultiVENT, a dataset of multilingual, event-centric videos grounded in text documents across five target languages. MultiVENT includes both news broadcast videos and non-professional event footage"
NewsNet,https://github.com/NewsNet-Benchmark/NewsNet,https://openaccess.thecvf.com/content/CVPR2023/html/Wu_NewsNet_A_Novel_Dataset_for_Hierarchical_Temporal_Segmentation_CVPR_2023_paper.html,2023,Hierarchical action recognition,Hierarchical news story events,1K videos,Both,News footage,T,"Video, text data","Four-granularity temporal intervals, class labels","we present two abstractive levels of temporal segmentations and study their hierarchy to the existing fine-grained levels. Accordingly, we collect NewsNet, the largest news video dataset consisting of 1,000 videos in over 900 hours, associated with several tasks for hierarchical temporal video segmentation."
NewsVideoQA,https://github.com/soumyasj/NewsVideoQA,https://arxiv.org/abs/2211.05588,2023,QA,News-related events involving text content,"8,600+ QA pairs on 3,000+ news videos",Both,News footage,T,Video,QA pairs,"we argue that textual information is complementary to the action and provides essential contextualisation cues to the reasoning process. To this end, we propose a novel VideoQA task that requires reading and understanding the text in the video."
Perception Test,https://github.com/google-deepmind/perception_test,https://proceedings.neurips.cc/paper_files/paper/2023/hash/8540fba4abdc7f9f7a7b1cc6cd60e409-Abstract-Datasets_and_Benchmarks.html,2023,QA,Reasoning-based events,11.6k real-world videos,Scripted,Dataset content,F,Video,"Multiple-choice + grounded QAs, object and point tracks, temporal actions, audio segments","We propose a novel multimodal video benchmark – the Perception Test – to evaluate the perception and reasoning skills of pre-trained multimodal models (e.g. Flamingo, SeViLA, or GPT-4). Compared to existing benchmarks that focus on computational tasks (e.g. classification, detection or tracking), the Perception Test focuses on skills (Memory, Abstraction, Physics, Semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities, to provide a comprehensive and efficient evaluation tool."
TextVR,https://github.com/callsys/TextVR/tree/main,https://arxiv.org/pdf/2305.03347.pdf,2023,Retrieval,"Street View (indoor), Street View (outdoor), Game, Sports, Driving, Activity, TV Show, and Cooking",42.2k sentence queries for 10.5k videos of 8 scenario domains,Both,Largely internet videos with heavy text content,T,Video,"Video queries, OCR content","Most existing cross-modal language-to-video retrieval (VR) research focuses on single-modal input from video, i.e., visual representation, while the text is omnipresent in human environments and frequently critical to understand video. To study how to retrieve video with both modal inputs, i.e., visual and text semantic representations, we firstly introduce a largescale and cross-modal Video Retrieval dataset with text reading comprehension, TextVR"
TVEE,_,https://link.springer.com/chapter/10.1007/978-3-031-30675-4_51,2023,Scene Parsing,"ACE2005 event schema, with three additional events: Contact.Speech, Disaster.Disaster and Accident.Accident.","7,598 text-video pairs",Both,International news footage,T,"Video, text","captions, triggers, event types, entities and argument roles (bounding boxes)","Event extraction aims to extract information of triggers and arguments from texts. Recent advanced methods leverage information from other modalities (e.g., images and videos) besides the texts to enhance event extraction. However, the different modalities are often misaligned at the event level, negatively impacting model performance. To address this issue, we firstly constructed a new multimodal event extraction benchmark Text Video Event Extraction (TVEE) dataset, containing 7,598 text-video pairs. The texts are automatically extracted from video captions, which are perfectly aligned to the video content in most cases."
VideoInstruct,https://mbzuai-oryx.github.io/Video-ChatGPT/,https://arxiv.org/pdf/2306.05424v1.pdf,2023,Instructions,Diverse coverage from spatial content to complex events,"100,000 video-instruction pairs",Unknown,Internet videos (ActivityNet subset),F,Video,"Captions, video-instruciton pairs","While there have been initial attempts for image-based conversation models, this work addresses the underexplored field of video-based conversation by introducing Video-ChatGPT. We introduce a new dataset of 100,000 video-instruction pairs used to train VideoChatGPT acquired via manual and semi-automated pipeline that is easily scalable and robust to label noise"
VEATIC,https://veatic.github.io/,https://openaccess.thecvf.com/content/WACV2024/html/Ren_VEATIC_Video-Based_Emotion_and_Affect_Tracking_in_Context_Dataset_WACV_2024_paper.html,2024,Emotion recognition,Emotions,124 video clips,Both,"Movies, documentaries, home videos",F,Video,Frame-by-frame valence/arousal labels,"Human affect recognition has been a significant topic in psychophysics and computer vision. However, the currently published datasets have many limitations. For example, most datasets contain frames that contain only information about facial expressions. Due to the limitations of previous datasets, it is very hard to either understand the mechanisms for affect recognition of humans or generalize well on common cases for computer vision models trained on those datasets. In this work, we introduce a brand new large dataset, the Video-based Emotion and Affect Tracking in Context Dataset (VEATIC), that can conquer the limitations of the previous datasets."